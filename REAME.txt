1) Полное задание из методички с вариантом

Задание 11: ResNet блоки с skip connections

Задача:  реализовать ResNet архитектуру с остаточными блоками

Требования:
Residual блок: Conv → BatchNorm → ReLU → Conv → BatchNorm + skip → ReLU
Bottleneck блок для уменьшения размерности
Адаптация skip connection при изменении размерности
ResNet-50 архитектура

Что нужно дополнить:
1. ResidualBlock с правильной skip connection логикой
2. BottleneckBlock для более глубоких сетей
3. Адаптацию skip connection при изменении размеров
4. ResNet50 архитектуру
5. Инициализацию весов

2) Алгоритм и архитектура работы НС по блокам
Ниже приведено пошаговое описание алгоритма работы нейросети (НС):

	1. Обработка входного изображения

	Вход: RGB изображение 224×224×3
	Первая свертка: 64 фильтра 7×7, stride=2 → уменьшение до 112×112
	BatchNorm + ReLU: Нормализация и нелинейность
	MaxPooling: 3×3, stride=2 → финальное уменьшение до 56×56

	2. Residual стадии (4 уровня)

	Каждая стадия состоит из Bottleneck блоков:

	Stage 1 (56×56)
	3 Bottleneck блока по 256 фильтров
	
	Сохраняют размер 56×56 (stride=1)

	Первый блок: identity mapping (без адаптации skip)

	Stage 2 (28×28)
	4 Bottleneck блока по 512 фильтров

	Первый блок: stride=2 → уменьшение до 28×28

	Skip connection адаптируется через conv 1×1

	Остальные 3 блока: stride=1 (сохраняют размер)

	Stage 3 (14×14)
	6 Bottleneck блоков по 1024 фильтров

	Первый блок: stride=2 → уменьшение до 14×14

	Skip connection адаптируется

	Остальные 5 блоков: сохраняют размер

	Stage 4 (7×7)
	3 Bottleneck блока по 2048 фильтров

	Первый блок: stride=2 → уменьшение до 7×7

	Skip connection адаптируется

	Остальные 2 блока: сохраняют размер

	3. Финальная классификация
	Global Average Pooling: 7×7×2048 → 2048 признаков

	Fully Connected слой: 2048 → 1000 классов (для ImageNet)

	Выход: вероятности принадлежности к классам
	СКИП-КОННЕКЦИИ (ОСТАТОЧНЫЕ СВЯЗИ)
	Два типа:

	Identity mapping — когда размерности совпадают, просто пропускаем тензор

	Projection mapping — когда нужно изменить размерность, применяем свертку 1×1

	Зачем нужны:

	Дают градиентам "короткий путь" для распространения

	Предотвращают исчезновение градиентов

	Позволяют сети изучать остаточные функции вместо полных преобразований

	ФИНАЛЬНАЯ ЧАСТЬ
	После всех сверточных блоков:

	Глобальное усредняющее пуллирование — превращает 7×7×2048 в вектор из 2048 чисел

	Полносвязный слой — преобразует 2048 признаков в 1000 вероятностей (для ImageNet)

	Выход — распределение вероятностей по классам

3) Ответ на контрольный вопрос:
Чем отличаются Type-I и Type-II (ultrafuzzy) нечёткие множества?
Type-I нечеткие множества используют обычные вещественные числа для степени принадлежности, а Type-II (ultrafuzzy) — сами степени принадлежности задаются нечеткими множествами.

1. Type-I (обычные) нечеткие множества:

Степень принадлежности — чёткое вещественное число из интервала [0, 1].
Пример: «Скорость 50 км/ч» может принадлежать множеству «средняя скорость» со степенью 0.8.

2. Type-II (ultrafuzzy) нечеткие множества:
Степень принадлежности сама нечёткая — задаётся не числом, а нечетким множеством на [0, 1].

Пример: Вместо «степень принадлежности = 0.8» говорят: «степень принадлежности — ВЫСОКАЯ», где «ВЫСОКАЯ» — нечеткое множество на [0,1].
Можно представить как «нечеткость в нечеткости» — неопределённость в значении функции принадлежности.

Вывод: Type‑II множества вводят дополнительный уровень неопределённости, что делает их мощнее, но значительно сложнее в вычислениях и интерпретации, чем Type‑I.